# All paths to different required data objects
images_dir: "/data/Deep_Learning_Datasets/celebA-HQ"
folder_distributed: False  # whether images are distributed among folders
log_dir: "/data/Deep_Learning_Datasets/_GENERATED/Pro-GAN/celebA-HQ/losses"
sample_dir: "/data/Deep_Learning_Datasets/_GENERATED/Pro-GAN/celebA-HQ/generated_samples"
save_dir: "/data/Deep_Learning_Datasets/_GENERATED/Pro-GAN/celebA-HQ/saved_models/"

# Hyperparameters for the Model
img_dims:
  - 1024
  - 1024

# Pro GAN hyperparameters
use_eql: True
depth: 9
latent_size: 512
learning_rate: 0.001
beta_1: 0
beta_2: 0.99
eps: 0.00000001
drift: 0.001
n_critic: 1
use_ema: True
ema_decay: 0.999

# Training hyperparameters:
epochs:
  - 27
  - 54
  - 54
  - 54
  - 54
  - 54
  - 54
  - 54
  - 54

# % of epochs for fading in the new layer
fade_in_percentage:
  - 50
  - 50
  - 50
  - 50
  - 50
  - 50
  - 50
  - 50
  - 50

batch_sizes:
  - 64
  - 64
  - 64
  - 64
  - 64
  - 64
  - 56
  - 24
  - 12

loss_function: "wgan-gp"  # loss function to be used

num_samples: 36
num_workers: 3
feedback_factor: 10   # number of logs generated per epoch
checkpoint_factor: 3  # save the models after these many epochs
